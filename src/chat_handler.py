

from typing import List, Dict, Generator, Optional, Any
import gradio as gr
from model_manager import ModelManager
import time, os, datetime
from prometheus_client import Counter, Summary

# Prometheus metrics definitions
REQUEST_COUNTER = Counter('app_requests_total', 'Total number of requests')
SUCCESSFUL_REQUESTS = Counter('app_successful_requests_total', 'Total number of successful requests')
FAILED_REQUESTS = Counter('app_failed_requests_total', 'Total number of failed requests')
REQUEST_DURATION = Summary('app_request_duration_seconds', 'Time spent processing request')

# Counter labeled by philosopher name to track which philosopher is being requested
PHILOSOPHER_COUNTER = Counter(
    'app_philosopher_requests_total',
    'Total number of requests per philosopher',
    ['philosopher']
)

# Counter for number of requests served by local model
LOCAL_MODEL_REQUESTS = Counter(
    'app_local_model_requests_total',
    'Total number of requests handled by the local model'
)

# Counter for number of requests served by API model
API_MODEL_REQUESTS = Counter(
    'app_api_model_requests_total',
    'Total number of requests handled by the API model'
)

# Summary for time taken to generate a response from local model
LOCAL_MODEL_REQUEST_DURATION = Summary(
    'app_local_model_request_duration_seconds',
    'Time spent generating responses from the local model'
)

# Completly generated using GitHub Copilot
def timing_decorator(func):
    def wrapper(*args, **kwargs):
        start = time.time()
        start_dt = datetime.datetime.now()
        print(f"[TIMING] {func.__name__} invoked at: {start_dt.strftime('%Y-%m-%d %H:%M:%S.%f')}")
        result = None
        try:
            result = func(*args, **kwargs)
            if hasattr(result, '__iter__') and not isinstance(result, (str, bytes, dict)):
                # If result is a generator, wrap it to time the iteration
                def gen_wrapper():
                    try:
                        for item in result:
                            yield item
                    finally:
                        end = time.time()
                        end_dt = datetime.datetime.now()
                        elapsed = end - start
                        print(f"[TIMING] {func.__name__} exited at: {end_dt.strftime('%Y-%m-%d %H:%M:%S.%f')}")
                        print(f"[TIMING] {func.__name__} total elapsed time: {elapsed:.4f} seconds")
                return gen_wrapper()
            else:
                return result
        finally:
            # Only log here for non-generator results and when result was assigned
            if result is not None and not (hasattr(result, '__iter__') and not isinstance(result, (str, bytes, dict))):
                end = time.time()
                end_dt = datetime.datetime.now()
                elapsed = end - start
                print(f"[TIMING] {func.__name__} exited at: {end_dt.strftime('%Y-%m-%d %H:%M:%S.%f')}")
                print(f"[TIMING] {func.__name__} total elapsed time: {elapsed:.4f} seconds")
    return wrapper

class ChatHandler:
    """Handles chat interactions and response generation"""
    
    def __init__(self, model_manager: ModelManager, config: Dict[str, Any], prompts: Dict[str, Any]):
        self.model_manager = model_manager
        self.config = config
        self.prompts = prompts
    
    def build_messages(self, message: str, history: List[Dict[str, str]], 
                      system_prompt: str) -> List[Dict[str, str]]:
        """Build message list from history and current message, using system_prompt from prompt_config"""
        messages = [{"role": "system", "content": system_prompt}]
        if self.config["history_limit"] > 0: # -1 means no memory
            messages.extend(history[-self.config["history_limit"]:])
        messages.append({"role": "user", "content": message})
        return messages
    
    def respond(self, 
                message: str, 
                history: List[Dict[str, str]], 
                gallery: Any,
                max_tokens: int, 
                temperature: float, 
                top_p: float, 
                use_local_model: bool,
                hf_token: Optional[gr.OAuthToken]) -> Generator[str, None, None]:
        """Generate response to user message, using prompt from prompt_config based on gallery selection"""

        # Determine selected philosopher from gallery input
        prompts = self.prompts

        selected_philosopher = None
        if gallery:
            # Gradio Gallery returns the selected image path as a string
            if isinstance(gallery, str):
                # Extract filename without extension
                selected_philosopher = os.path.splitext(os.path.basename(gallery))[0]
                
            elif isinstance(gallery, list) and len(gallery) > 0:
                # Sometimes Gallery returns a list of selected items
                item = gallery[0]
                if isinstance(item, str):
                    selected_philosopher = os.path.splitext(os.path.basename(item))[0]
                elif isinstance(item, (list, tuple)) and len(item) > 0:
                    selected_philosopher = os.path.splitext(os.path.basename(item[0]))[0]

        # Fallback: use first key in prompt_config if nothing selected
        if not selected_philosopher and prompts:
            selected_philosopher = next(iter(prompts.keys()))

        # Get introduction/system prompt
        system_prompt = ""
        if selected_philosopher and prompts and selected_philosopher in prompts:
            system_prompt = prompts[selected_philosopher].get("introduction", "")
        messages = self.build_messages(message, history, system_prompt)

        # Start metrics for this request
        REQUEST_COUNTER.inc()
        if selected_philosopher:
            try:
                PHILOSOPHER_COUNTER.labels(philosopher=selected_philosopher).inc()
            except Exception:
                # Labels may fail if invalid; ignore metric failure
                pass

        # Increment local/api specific counters
        if use_local_model:
            LOCAL_MODEL_REQUESTS.inc()
            gen = self._handle_local_model(messages, max_tokens, temperature, top_p)
        else:
            API_MODEL_REQUESTS.inc()
            gen = self._handle_api_model(messages, max_tokens, temperature, top_p, hf_token)

        # Consume the generator and return a final string to Gradio
        # Gradio's ChatInterface expects a message-like object (not a raw generator)
        full_response = ""
        with REQUEST_DURATION.time():
            try:
                for chunk in gen:
                    # Append chunk to the accumulating response. Generators may
                    # yield incremental strings or cumulative text; append whatever
                    # is produced.
                    if isinstance(chunk, str):
                        full_response += chunk
                    else:
                        full_response += str(chunk)
                SUCCESSFUL_REQUESTS.inc()
            except Exception:
                FAILED_REQUESTS.inc()
                raise

        return full_response
    
    @timing_decorator
    def _handle_local_model(self, messages: List[Dict[str, str]], max_tokens: int, 
                           temperature: float, top_p: float) -> Generator[str, None, None]:
        """Handle local model response generation"""
        print("[MODE] local")
        local_model = self.model_manager.local_model
        # Check if model is still loading
        if local_model.is_loading():
            queued_data = {
                'messages': messages,
                'max_tokens': max_tokens,
                'temperature': temperature,
                'top_p': top_p,
                'use_local_model': True
            }
            self.model_manager.queue_message(queued_data)
            yield self.config["messages"]["loading_message"]
            while local_model.is_loading():
                time.sleep(1)
            if not local_model.is_ready():
                yield self.config["messages"]["model_load_failed"]
                return
            yield self.config["messages"]["model_ready"]
        elif not local_model.is_ready():
            yield self.config["messages"]["model_load_failed"]
            return
        try:
            # Time only the local model generation (not the loading messages)
            with LOCAL_MODEL_REQUEST_DURATION.time():
                for token in local_model.generate(
                    messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=top_p,
                ):
                    yield token
        except Exception as e:
            yield f"Error generating response: {str(e)}"
    
    @timing_decorator
    def _handle_api_model(self, messages: List[Dict[str, str]], max_tokens: int,
                         temperature: float, top_p: float, 
                         hf_token: Optional[gr.OAuthToken]) -> Generator[str, None, None]:
        """Handle API model response generation"""
        print("[MODE] api")
        # Prefer token from Gradio login if provided, otherwise use environment variable
        token = None
        if hf_token and getattr(hf_token, "token", None):
            token = hf_token.token
        else:
            token = os.environ.get("HF_TOKEN")

        if not token:
            # No token available; instruct user/admin to set HF_TOKEN
            yield self.config["messages"]["login_required"]
            return

        try:
            yield from self.model_manager.api_model.generate(
                messages,
                hf_token=token,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p
            )
        except Exception as e:
            yield f"Error generating response: {str(e)}"
